# -*- coding: utf-8 -*-
"""NASA- Astroid classification on nearest earth object.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZCinArKGu5HSLrDSJ0v_mr1RhsVWjcOK
"""

from google.colab import drive
drive.mount('/content/drive')

# Important libraries used
# Basic libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

# For statistical tests
import scipy.stats as stats

# For statistical tests
import scipy.stats as stats

# For model building
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# For evaluation
from sklearn.metrics import f1_score,accuracy_score,confusion_matrix,ConfusionMatrixDisplay,classification_report
from sklearn.metrics import recall_score,precision_score

# for preprocessing and splitting of the data
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.model_selection import train_test_split

df=pd.read_csv('/neo.csv')

df

"""EDA"""

df1=df.copy()

df1

# First five rows of the data
df.head()

# info of the data
df.info()

# Summary statistics
df.describe().T

# Categorical summary statitics
df.describe(include=object).T

# Checking for duplicate rows
df.duplicated().value_counts()
print("Data has no duplicate rows")

# Checking for missing values
df.isnull().sum()
# Data has no missing values

# Corelation Matrix
df.corr()

# Vizualization of the correlation matrix
sns.heatmap(df.corr(),vmin=-1,vmax=1,cmap='YlGnBu')
plt.show()

# Dropping some redundant column:
# We will drop the orbiting_body and sentry_object columns because these 2 only have one kind of values namely,Earth and False.
# We would also be dropping the id column as well
df.drop(columns=['orbiting_body','sentry_object','name','id'],inplace=True)

df

## Column: est_diameter_min
sns.distplot(df.est_diameter_min,color='black')
plt.show()

"""We see that most of the asteroids are less than 2.5 kms in estimated minimum diameter. There are records of higher diameter asteroids as well(20+ km). These could be more hazardous than the smaller ones. We would check this further"""

sns.boxenplot(x='hazardous',y='est_diameter_min',data=df)
plt.show()

#We see that non hazardous objects have the highest minimum diameter but hazardous objects on average have a higher minimum diameter

# est_diameter_max coloumn
sns.distplot(df.est_diameter_max,color='black')
plt.show()

#Similar to our observation on est_diameter_min we see that most objects have estimated max diameter less than 2.5 kms

sns.boxenplot(x='hazardous',y='est_diameter_max',data=df)
plt.show()

#We see that non hazardous objects have the highest maximum diameter but hazardous objects on average have a higher maximum diameter

# relative_velocity
sns.distplot(df.relative_velocity)
plt.show()

#We see that most objects have a relative velocity between 0 and 150000 km/hr

sns.boxenplot(x='hazardous',y='relative_velocity',data=df)
plt.show()

#We see that the maximum relative velocity of non-hazardous objects in higher thn hazardous objects, but on average the relative velocity of hazardous objects is higher

#miss_distance
sns.distplot(df.miss_distance)
plt.show()

#we see that most objects miss Earth by a distance between 0.1x10^7 and 7.8x10^7

sns.boxplot(x='hazardous',y='miss_distance',data=df)
plt.show()

#We see that the average miss distance of hazardous objects is slightly higher than miss distance of non-hazardous objects

#absolute_magnitude
sns.distplot(df.absolute_magnitude)
plt.show()

#We see that the absolute magnitude(intrinsic luminosity) of most objects is between 15 and 30 units

sns.boxplot(x='hazardous',y='absolute_magnitude',data=df)
plt.show()

#we see that non-hazardous objects have a much higher absolute magnitude compared to hazardous objects

sns.countplot(x='hazardous',data=df)
plt.show()

"""We see that the number of hazardous objects is extremely small compared to our non-hazardous objects. This shows that our target variable is highly imbalanced

**Conclusion from the Exploratory Data Analysis

We should be focused more towards larger asteriods as they seem to be more hazardous than smaller asteroids.
We see that most objects have a relative velocity between 0 and 150000 km/h. Also Hazardous objects seems to be having a higher relative velocity on average.
We see that most objects miss Earth by a distance between 0.1x10^7 and 7.8x10^7 units. Also, Hazarous objects seem to be missing earth by a higher distance on average
We see that the absolute magnitude(intrinsic luminosity) of most objects is between 15 and 30 units. Also the intrinsic luminosity of hazardous objects is much lower than non-hazardous objects. This entails that it is much harder to detect hazardous objects than non-hazardous objects
By the current repository of data, hazardous objects seem to be much less in number compared to non-hazardous objects. This could be followed from the previous point due to hazardous objects being difficult to detect with a much a lower intrisic luminosity.

# pandas profiling
"""

pip install pandas_profiling



import pandas as pd
from pandas_profiling import ProfileReport

profile=ProfileReport(df, title="EDA_Nearest_earth_object")

profile

profile

# Heatmap
sns.heatmap(df.corr(),annot=True)
plt.show()

# Feature Engineering

# Before proceeding to our feature selection we would do some feature engineering if possible

# Taking the average of est_diameter_min and est_diameter_max to make a new feature est_diameter_avg
df['est_diameter_avg']=(df.est_diameter_max+df.est_diameter_min)/2

df['est_diameter_avg']

df

# Splitting the data into train and test

# Mapping the target variable to 1 and 0
df.hazardous=df.hazardous.map({True:1,False:0})

df['hazardous']

# Splitting the data into x and y
x=df.drop(columns='hazardous')
y=df.hazardous

# Train test split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,stratify=y)

# Shape of the train and test sets
x_train.shape,y_train.shape,x_test.shape,y_test.shape

x_train

x_test

y_train

y_test

# Creating an Evaluation Function / Confusion matrix

def evaluation(y_test,y_pred):
    print('Accuracy Score:',accuracy_score(y_test,y_pred))
    print('f1_score:',f1_score(y_test,y_pred))
    print('Precision:',precision_score(y_test,y_pred))
    print('Recall:',recall_score(y_test,y_pred))
    print('Classification report:\n',classification_report(y_test,y_pred))
    cm=confusion_matrix(y_test,y_pred)
    ConfusionMatrixDisplay(cm).plot()
    plt.title('Confusion matrix for the given prediction')
    plt.show()

# Our evaluation fucntion is now created

# logistics Regression

# Since Logistic Regression is a distance based algorithm we would need to scale our train and test sets
# before procedding to our model fitting
# For this we would use Standard scaler

sc=StandardScaler()

x_train_s=pd.DataFrame(sc.fit_transform(x_train),columns=x_train.columns)
x_test_s=pd.DataFrame(sc.transform(x_test),columns=x_test.columns)

# Our train and test sets are now scaled

x_train_s

x_test_s

# We will now proceed to our modelling
lr=LogisticRegression() # Machine instance

lr_model=lr.fit(x_train_s,y_train)
y_pred=lr_model.predict(x_test_s)

#Evalution

# We will see a few metrics and check how our model is performing
evaluation(y_test,y_pred)

"""# Desicion tree"""

dtree=DecisionTreeClassifier()

# Evalutaion of the model
evaluation(y_test,y_pred)

"""# Random Forest"""

rf=RandomForestClassifier()

# Fitting and predicting
y_pred=rf.fit(x_train,y_train).predict(x_test)

#Evalution

# evaluation of the model
evaluation(y_test,y_pred)

"""# Conclusion

The Accuracy with Random Forest is better than Decesion Tree and Logistic regression
The astreoid with diameter less than 2.5 km is non_hazardous .
The astreoid with diameter 20+ km are hazardous in nature

## Hyper-parameter Tuning using GridSearchCV
"""

x

y

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

x=scaler.fit_transform(x)
x

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)

from sklearn.tree import DecisionTreeClassifier
clf=DecisionTreeClassifier()

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)

param_dist={
    "criterion":["gini","entropy"],
    "max_depth":[1,2,3,4,5,6,7,8,9,None]
}

from sklearn.model_selection import GridSearchCV
grid=GridSearchCV(clf,param_grid=param_dist,cv=10,n_jobs=-1)

grid.fit(x_train,y_train)

grid.best_estimator_

grid.best_score_

grid.best_params_